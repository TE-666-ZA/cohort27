# Lesson 05

## 00. Разбор ДЗ

* Закомментировать весь код - `Ctrl + A`, затем `Ctrl + /`
* Переменные называем осмысленно только английскими словами с маленькими. `camelCase`
* Само название класса/файла - `PascalCase`, только английские слова!
* Переменную можно переимновать один раз и сразу с помощью `Shift + F6` и `Rename code occureences`
* Всегда используем `Ctrl + Alt + L` - это форматирует код, особенно перед отправкой на проверку.
* Убрать строку `Ctrl + Y`


## 01. Целые числа и их представление в компьютере

* Что нужно знать?

1. Что такое байт и бит
2. Преобразования из двоичной системы в десятичную и обратно
3. Сколько разных чисел можно выразить N-битами
4. Сколько чисел может хранить тип `byte` и `int`?
5. Как хранятся отрицательные числа
6. Что такое дополнительный код?
7. Что такое знаковый бит?
8. Переполнение (число становится отрицательным если вы выходите за диапазон, или в положительные, если в другую сторону)

* Байт (byte) - минимальная единица информации в памяти компьютера. Байт состоит из 8 бит. Каждый бит принимает значение либо 0 либо 1.

```
byte b = 10; // 1 байт
int i = 378; // 4 байта
long l = 15; // 8 байт
```

![image](https://raw.githubusercontent.com/ait-tr/cohort27/main/basic_programming/lesson_05/img/1.png)

## Немного математики

```
5^3 = 5 * 5 * 5 = 125
5^1 = 5
5^0 = 1
```

## Десятичная система счисления:

```
32113 = 3 * 10 000 + 2 * 1 000 + 1 * 100 + 1 * 10 + 3 * 1

43210
32113 = 3 * 10^4 + 2 * 10^3 + 1 * 10^2 + 1 * 10^1 + 3 * 10^0
```

## Двочиная система счисления

```
2^0 = 1
2^1 = 2
2^2 = 4
2^3 = 8
2^4 = 16
2^5 = 32
2^6 = 64
2^7 = 128
2^8 = 256
2^9 = 512
2^10 = 1024
2^11 = 2048
2^12 = 4096
```

* Преобразование из двоичной системы счисления в десятичную систему счисления.

```
76543210
01101011(2) -> 0 * 2^7 + 1 * 2^6 + 1 * 2^5 + 0 * 2^4 + 1 * 2^3 + 0 * 2^2 + 1 * 2^1 + 1 * 2^0 = 64 + 32 + 8 + 2 + 1 = 107(10)
```

* Преобразование из десятичной системы счисления в двоичную систему счисления.

```
107|2|1    107 = 53 * 2 + (1)
 53|2|1     53 = 26 * 2 + (1)
 26|2|0     26 = 13 * 2 + (0)
 13|2|1     13 =  6 * 2 + (1)
  6|2|0      6 =  3 * 2 + (0)
  3|2|1      3 =  1 * 2 + (1)
  1

107(10) -> 01101011
```

* Максимальное положительное число, которое можно положить в байт

```
 1111111 = 127

1111111 + 1 = 10000000

10000000 = 1 * 2^7 + 0 ... + 0 = 128
```

* Сколько всего разных чисел может поместиться в N-бит?

```
0 + 1 = 1
1 + 0 = 1
1 + 1 = 10 -> 2(10)
1 + 1 + 1 = 11 -> 3(10)

N = 1

0(2) -> 0(10)
1(2) -> 1(10)

N = 2

00(2) -> 0(10)
01(2) -> 1(10)
10(2) -> 2(10)
11(2) -> 3(10)

N = 3

000(2) -> 0(10)
001(2) -> 1(10)
010(2) -> 2(10)
011(2) -> 3(10)
100(2) -> 4(10)
101(2) -> 5(10)   101 -> 1 * 2^2 + 0 * 2^1 + 1 * 2^0 = 4 + 0 + 1 = 5
110(2) -> 6(10)
111(2) -> 7(10)

При:
N = 1, count = 2 = 2^1, min = 0, max = 1
N = 2, count = 4 = 2^2, min = 0, max = 3
N = 3, count = 8 = 2^3, min = 0, max = 7

при N = N, count = 2^N, min = 0, max = 2^N - 1

Например

Подробнее:

У меня есть 5 бит, сколько разных комбинаций 0 и 1 я могу сделать, используя 5 бит

count = 2^5 = 32, min = 0, max = 2^5 - 1 = 31

00000 - 0
00001 - 1 
00010 - 2
00011 - 3
00100 - 4
00101 - 5
00110 - 6
.....
11111 - 31

N = 8 бит 

count - 2^8 = 256 
min - 0
max - 255
```

* В java несмотря на то, что `byte` занимает 8 бит, используются только 7 из них. Поэтому в `byte max = 127, min = -128`, `count = 256` 

```
-128
-127
-126
-125
...
0
1
2
...
127
```

![image](https://raw.githubusercontent.com/ait-tr/cohort27/main/basic_programming/lesson_05/img/2.png)

* В Java все типы знаковые, поэтому диапазон делится пополам и получаем итоговую формулу:

```
N - бит

count = 2^N,
min = -2^(N-1)
max = 2^(N-1) - 1

При N = 8 (byte)

count = 2^8 = 256
min = -2^(8 - 1) = -2^7 = -128
max = 2^(8 - 1) - 1 = 128 - 1 = 127

Для N = 32 (int, int - 4 байта, 4 байта * 8 - 32 бита)

count = 2^32
min = -2^31
max = 2^31 - 1
```

### Хранение отрицательных чисел в компьютере

* `-27(10)`, как его сохранить в компьютере?

* Оно хранится в `дополнительном коде`

* Как получить дополнительный код?

* Алгоритм:

```
-27(10)

1. Берем модуль числа

-27 -> 27

2. Преобразуем его в двоичный вид

27(10) = 11011(2) 

27|2|1
13|2|1
 6|2|0
 3|2|1
 1

3. Дополняем нулями (например до 1-го байта)

11011 -> 00011011

4. Делаем обратный код (заменяем нули на единицы и наоборот)

00011011 -> 11100100

5. Делаем дополнительный код (просто прибавляем 1)

11100100 + 1 = 11100101

11100101 -> это число -27, которое хранится в компьютере
```

* Комьютер понимает, что число отрицательное, потому что старший бит (первый слева, знаковый) равен 1-е.

* Единица помогает компьютеру понять, что число записано в дополнительном коде и оно отрицательное

### Дополнительно

```
byte c = b + 2;
```

Почему ошибка? Потому что 2 воспринимается как `int`, в итоге `b + 2` будет иметь тип `int`, а `int` нельзя преобразовать к `byte`.

### Переполнение 

```java
byte b = 127;
byte c = (byte) (b + 1);
System.out.println(c); // -128
```

```
0 + 1 = 1
1 + 0 = 1
1 + 1 = 10 -> 2(10)
1 + 1 + 1 = 11 -> 3(10)

127(10) -> 01111111

Прибавляем единицу:
1111111   - в уме
--------
01111111
00000001
--------
10000000

1111111 + 1 = 10000000

А мы помним, что первый бит отвечает за знак, следовательно
это число воспринимается компьютером как отрицательное.

10000000
```

![image](https://raw.githubusercontent.com/ait-tr/cohort27/main/basic_programming/lesson_05/img/3.png)

## 02. Введение в символьный тип

* Компьютер хранит все символы в виде чисел.

* Таблица ASCII - американский стандарт кода для обмена информации. Каждому символу ставится в соответствие число, которое кодирует этот символ.

Например:

`A` - 65
`B` - 66
`*` - 42
` ` - 32
`a` - 97
`b` - 98
`0` - 48
`1` - 49

* Как это работает? В файле на диске, где есть текст с буквой A, на самом деле нет буквы A, там записано число `65`, а уже когда информация выводится на экран, там появляется буква `A`.

* Таблица ASCII хранит 128 символов, которые уже зашиты в таблицу, а остальные 128 отводятся под локальные алфавиты (например русский язык).

* Сколько занимает места такой символ в байтах? - 1 байт (0...255).

* Какие минусы? - текст на русском языке в Германии будет отображться неправильно, потому на ноутбуках в Германии нет кириллицы, там в оставшихся 128 позициях лежат немецкие буквы и символы.

* Поэтому есть решение - используют таблицу `Unicode`.

* Символ в Unicode занимает 2 байта, следовательно можно закодировать 2^16 степени символов, а это примерно 65 000, т.е. любой алфавит мира и туда реально зашиты все алфавиты, например русская `A` соответствует коду `1040`

* Но! Первые 127 символов Unicode такие же, как в ASCII.

Группа непечатных символов (символы, которые не видно):

`\n` - переход на новую строку, код `10`
`\t` - символ табуляции (большой пробел), код `9`    

[Таблица кодировки](https://raw.githubusercontent.com/ait-tr/cohort27/main/basic_programming/lesson_05/img/1.webp)

## Словарик

* Review - проверка кода